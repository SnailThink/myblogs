### 前言

距离上次公众号发文间隔的时间不知不觉已经过去一个月了[原谅我偷懒了]。本来应该是2月底写这篇文章的，

竟然拖到了现在。俺就把2月的总结和3月的总结一起写了。给自己敲个警钟，不能在继续拖拖拉拉。

### 反思

最近一直没有更新公众号，其实还是写了不少的东西的，但是基本都是笔记杂记，还没有整体的整理出来。

后面我整理出来，在进行发出来。

![5.png](https://mmbiz.qpic.cn/mmbiz_png/r3Tib18c3hBXZc0Umb8XhylBrqGE9E3PxQKRqzU7nibsWBzv02FQpiaEadjwnvZTokHvRBw9P3xiamNjod4ibNRSnsQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)5.png

![6.png](https://mmbiz.qpic.cn/mmbiz_png/r3Tib18c3hBXZc0Umb8XhylBrqGE9E3PxJcsZqkClibeFcr6656olJEGIMnf4IWXiaae0aBp9HrHbDrCXN1st0oJw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)6.png

其实大部分时间都在准备这些东西，但是做的还是不够多吧，还是不太行，回过头来看自己最近写的东西，感觉质

量还是太差，所以感觉更新公众号文章可能比较慢了，因为发现在写东西的时候就写着写着写不下去了，由于写的

质量不行，那就就没有人看，那么我就没有动力写，也就质疑自己了。也就坚持不下去了，这样就造成了一个恶性

循环，还是要多看书，写出高质量的文章，奥利给【手动狗头】



### 爬虫

#### 获取链家二手房

最近看了哈二手房，顺便学习了哈python爬取链家二手房信息。刚需的抓紧时间买。留给我们的机会不多了。

[最近还被狠狠鄙视了一次，说我关中人没有魄力，买个房首付不就30,40w，有啥好慌的，我拖大家后腿了]

用python确实挺方便的，可以解决很多实际的问题。

```python
# 爬取链家二手房import requestsimport refrom bs4 import BeautifulSoupi=0with open('./lianjia/lianjia.csv','a') as file:     for j in range(1,5,1):          i+=1          url = 'https://bj.lianjia.com/ershoufang/pg+str(i)'          headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'}          resp = requests.get(url,headers = headers).text          # print(resp)          soup = BeautifulSoup(resp,'lxml')          infos = soup.find('ul',{'class':'sellListContent'}).find_all('li')          # print(infos)          for info in infos:               name = info.find('div',{'class':'title'}).find('a').get_text()               price = info.find('div',{'class':'totalPrice'}).find('span').get_text()               unit = info.find('div',{'class':'unitPrice'}).find('span').get_text()               address = info.find('div',{'class':'address'}).get_text()               print(unit)               area = re.split(r'\|',address)[2]               print(area)               file.write('{},{},{},{},{}\n'.format((name),(unit),(price),(address),area))
```

#### 爬取豆瓣top 250

```python
# 爬取豆瓣top250import requestsimport jsonfrom bs4 import BeautifulSoup# 获取源码def get_html(url):    response = requests.get(url)    if response.status_code == 200:        return response.text    else:        print("访问失败")# 解析源码def parse_html(html):    soup = BeautifulSoup(html, 'lxml') # 创建BeautifulSoup对象，使用lxml解析库    items = soup.find_all(name='li')  # 查询名称为li的元素，以列表形式输出    for item in items[19:]:    # 注意这里的[19:]，对网页源码分析后发现前19个li标签中并没有我们需要的数据，所以将其排除在外        yield {            # 利用find()定位我们需要的数据，并作为生成器元素            'title': item.span.get_text(),            'index': item.find(name='em').text,            'image': item.find(name='a')['href'],            'quote': item.find(class_="inq").text,            'score': item.find(class_="rating_num").text        }        # 构造生成器，作为函数的返回结果# 将数据转化为json字符串并写入到文件中def write_to_file(content):    with open("movies.txt", 'a', encoding='utf-8') as file:  # 以追加的权限打开文件movies.txt        file.write(json.dumps(content, ensure_ascii=False) + '\n')  # ensure_ascii设为False，保证输出是中文形式，而不是ASCII编码        # json.dumps()序列化时默认对中文使用ascii编码def main(start):    url = 'https://movie.douban.com/top250?start=' + str(start) + '&filter'  # 定位top250的url    html = get_html(url)    for item in parse_html(html):        print(item)        write_to_file(item)# 执行函数，完成爬取if __name__ == '__main__':    for i in range(10):        main(i * 25)
```

### 近况

- 能看到这里的都是人才啊，我写的乱七八糟的没有啥重点。反正也没有人看见
- 最近的状态不太好，导致文章质量以及产出都大大的下降，但是看看11号发布文章，没有一个人看!

![9.png](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)9.png

这么好的文章竟然没人看，你们竟然不看。白嫖都不会。。。。。

- 我身边的小伙伴，一个，两个，三个……慢慢的离职了。e聊了很多，感慨万千，从当时的一起入职，

一起加班，一起happy，到现在就只剩下我一个了。最近的行情还是不太好，[其实就是我自己不行，实力太差]

总的来说离职一定要找好下家呦,他们去的公司还不错.

- 在谈谈最近的股票，大家都说可以抄底。你品品下面的这个。我的内心是绝望的，被割了一波又一波韭菜。

![10.png](https://mmbiz.qpic.cn/mmbiz_png/r3Tib18c3hBXZc0Umb8XhylBrqGE9E3PxFibXgonVcFaatiar2CggPPG5Ujmc1zpxXialtdRib6Eb93icekOUXObR4TQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)