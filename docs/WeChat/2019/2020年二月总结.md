### 前言

距离上次公众号发文间隔的时间不知不觉已经过去一个月了[原谅我偷懒了]。本来应该是2月底写这篇文章的，

竟然拖到了现在。俺就把2月的总结和3月的总结一起写了。给自己敲个警钟，不能在继续拖拖拉拉。



### 反思

最近一直没有更新公众号，其实还是写了不少的东西的，但是基本都是笔记杂记，还没有整体的整理出来。

后面我整理出来，在进行发出来。

![5.png](https://user-gold-cdn.xitu.io/2020/3/24/1710c537086ec482?w=1260&h=380&f=png&s=27570)

![6.png](https://user-gold-cdn.xitu.io/2020/3/24/1710c527df189a2c?w=1560&h=989&f=png&s=139724)

其实大部分时间都在准备这些东西，但是做的还是不够多吧，还是不太行，回过头来看自己最近写的东西，感觉质

量还是太差，所以感觉更新公众号文章可能比较慢了，因为发现在写东西的时候就写着写着写不下去了，由于写的

质量不行，那就就没有人看，那么我就没有动力写，也就质疑自己了。也就坚持不下去了，这样就造成了一个恶性

循环，还是要多看书，写出高质量的文章，奥利给【手动狗头】

​	 

### 爬虫



#### 获取链家二手房

最近看了哈二手房，顺便学习了哈python爬取链家二手房信息。刚需的抓紧时间买。留给我们的机会不多了。

[最近还被狠狠鄙视了一次，说我关中人没有魄力，买个房首付不就30,40w，有啥好慌的，我拖大家后腿了]

用python确实挺方便的，可以解决很多实际的问题。

```python
# 爬取链家二手房
import requests
import re
from bs4 import BeautifulSoup

i=0

with open('./lianjia/lianjia.csv','a') as file:
     for j in range(1,5,1):
          i+=1
          url = 'https://bj.lianjia.com/ershoufang/pg+str(i)'
          headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'}
          resp = requests.get(url,headers = headers).text
          # print(resp)

          soup = BeautifulSoup(resp,'lxml')
          infos = soup.find('ul',{'class':'sellListContent'}).find_all('li')
          # print(infos)

          for info in infos:
               name = info.find('div',{'class':'title'}).find('a').get_text()
               price = info.find('div',{'class':'totalPrice'}).find('span').get_text()
               unit = info.find('div',{'class':'unitPrice'}).find('span').get_text()
               address = info.find('div',{'class':'address'}).get_text()
               print(unit)
               area = re.split(r'\|',address)[2]
               print(area)
               file.write('{},{},{},{},{}\n'.format((name),(unit),(price),(address),area))
```



#### 爬取豆瓣top 250

```python
# 爬取豆瓣top250
import requests
import json
from bs4 import BeautifulSoup


# 获取源码
def get_html(url):
    response = requests.get(url)
    if response.status_code == 200:
        return response.text
    else:
        print("访问失败")


# 解析源码
def parse_html(html):
    soup = BeautifulSoup(html, 'lxml') # 创建BeautifulSoup对象，使用lxml解析库
    items = soup.find_all(name='li')  # 查询名称为li的元素，以列表形式输出
    for item in items[19:]:
    # 注意这里的[19:]，对网页源码分析后发现前19个li标签中并没有我们需要的数据，所以将其排除在外
        yield {
            # 利用find()定位我们需要的数据，并作为生成器元素
            'title': item.span.get_text(),
            'index': item.find(name='em').text,
            'image': item.find(name='a')['href'],
            'quote': item.find(class_="inq").text,
            'score': item.find(class_="rating_num").text
        }
        # 构造生成器，作为函数的返回结果


# 将数据转化为json字符串并写入到文件中
def write_to_file(content):
    with open("movies.txt", 'a', encoding='utf-8') as file:  # 以追加的权限打开文件movies.txt
        file.write(json.dumps(content, ensure_ascii=False) + '\n')  # ensure_ascii设为False，保证输出是中文形式，而不是ASCII编码
        # json.dumps()序列化时默认对中文使用ascii编码


def main(start):
    url = 'https://movie.douban.com/top250?start=' + str(start) + '&filter'  # 定位top250的url
    html = get_html(url)
    for item in parse_html(html):
        print(item)
        write_to_file(item)


# 执行函数，完成爬取
if __name__ == '__main__':
    for i in range(10):
        main(i * 25)


```



### 近况

- 最近的状态不太好，导致文章质量以及产出都大大的下降，但是看看11号发布文章，没有一个人看!

![9.png](https://user-gold-cdn.xitu.io/2020/3/24/1710c70a64739832?w=1205&h=146&f=png&s=42268)

这么好的文章竟然没人看，你们竟然不看。白嫖都不会。。。。。



- 能看到这里的都是人才啊，我写的乱七八糟的没有啥重点。~~反正也没有人看见~~

-  我身边的小伙伴，一个，两个，三个……慢慢的离职了。 e聊了很多，感慨万千，从当时的一起入职，

一起加班，一起happy，到现在就只剩下我一个了。最近的行情还是不太好，[其实就是我自己不行，实力太差]

总的来说离职一定要找好下家呦,他们去的公司还不错.



- 在谈谈最近的股票，大家都说可以抄底。你品品下面的这个。我的内心是绝望的，被割了一波又一波韭菜。

![10.png](https://user-gold-cdn.xitu.io/2020/3/24/1710c7ab36246c89?w=1125&h=1332&f=png&s=521611)



